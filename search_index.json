[
["index.html", "Benchmarking methods for estimating immune cell abundance from bulk RNA-sequencing data (supplementary information) 1 Introduction", " Benchmarking methods for estimating immune cell abundance from bulk RNA-sequencing data (supplementary information) manuscript in preparation Gregor Sturm\\(^{1,2}\\) Tatsiana Aneichyk\\(^1\\) \\(^1\\)Pieris Pharmaceuticals GmbH, Lise-Meitner-Straße 30, 85354 Freising, Germany \\(^2\\)Experimental Bioinformatics, TUM School of Life Sciences Weihenstephan, Technical University of Munich, Maximus-von-Imhof-Forum 3, 85354 Freising, Germany 2018-10-12 1 Introduction This document is supplementary information for Sturm, G. and Aneichyk T. Benchmarking methods for estimating immune cell abundance from bulk RNA-sequencing data, Manuscript in preparation. This document elaborates step-by-step how we derive our results, shows supplementary figures to support our study and serves as an open-source pipeline to reproduce our results. The source code and instructions how to run the benchmark are available from GitHub: grst/immune_deconvolution_benchmark "],
["input-data.html", "2 Input data 2.1 cell type hierarchy 2.2 Single cell data for simulated mixtures 2.3 Immune cell reference samples 2.4 8 PBMC samples from Hoek et al. (2015) 2.5 3 ovarian cancer ascites samples from Schelker et al. (2017) 2.6 4 metastatic melanoma samples from Racle et al. (2017) 2.7 Data sanity checks", " 2 Input data In this chapter, we load and preprocess the datasets we use in this study Dataset Reference A dataset of more than 11,000 single cancer and immune cells, classified by cell type Schelker et al. (2017) 50 Immune cell reference samples from 5 studies Curated by Finotello et al. (2017) 3 ovarian cancer ascites samples (RNAseq + FACS) Schelker et al. (2017) 8 PBMC samples (RNAseq + FACS) Hoek et al. (2015) 4 metastatic melanoma samplese (RNAseq + FACS) Racle et al. (2017) 2.1 cell type hierarchy We use a hierachy of immune cell types to map the cell types between different methods and datasets. The following figure shows this hierarchy visualized as a tree Figure 2.1: Hierarchy of immune cell types used for mapping cell types between methods and datasets. 2.2 Single cell data for simulated mixtures In this study, we make use of the single cell dataset curated by Schelker et al. (2017). They aggregated single cell sequencing data from different sources resulting in a set of more than 11,000 single cells. They classified the cells using at set of 45 marker genes into 12 categories: 2 cancer types (Melanoma cell, Ovarian carcinoma cell), 7 immune cells (B cell, T cell CD8+, T cell CD4+ (non-regulatory), Macrophage/Monocyte, T cell regulatory (Tregs), Dendritic cell, NK cell), 2 other cells (Cancer associated fibroblast, Endothelial cell) and Unknown cells which could not have been classified unambiguously. Unknown cells are excluded from the downstream analysis. The dataset consists of single cells from PBMC, melanoma and ovarian cancer ascites. As we are interested in the deconvolution of cancer samples, we exclude the PBMC cells from all downstream analyses. Table 2.1: The 11434 single cells by cell type cell_type n B cell 646 Cancer associated fibroblast 132 Dendritic cell 140 Endothelial cell 71 Macrophage/Monocyte 2227 Melanoma cell 1310 NK cell 198 Ovarian carcinoma cell 300 PBMC 3942 T cell CD4+ (non-regulatory) 1196 T cell CD8+ 1130 T cell regulatory (Tregs) 142 Figure 2.2: tSNE-clustering of the ~12,000 single cells from Schelker et al. (2017). 2.3 Immune cell reference samples RNAseq samples of pure immune cells of 10 types from 5 studies curated by Finotello et al. (2017). Table 2.2: List of immune cell reference samples 2.4 8 PBMC samples from Hoek et al. (2015) Table 2.2: Flow cytometry estimates of Hoek et al. 2.5 3 ovarian cancer ascites samples from Schelker et al. (2017) Each sample has two technical replicates. We merge the two replicates by taking the mean for each gene. Table 2.2: Flow cytometry estimates of Schelker et al. The samples have also been profiled by single cell RNA sequencing. The following table shows the cell count for each sample. Table 2.3: Single cell count per ovarian cancer ascites sample. donor sum(cell_count) 7873M 864 7882M 902 7892M 773 2.6 4 metastatic melanoma samples from Racle et al. (2017) Table 2.2: Flow cytometry estimates of Racle et al. 2.7 Data sanity checks Here, we plot the distributions of the different gene expression datasets to ensure that everything looks like we expect it to, e.g. if all datasets are on non-log scale. Figure 2.3: Histogram of gene expression data of all datasets Figure 2.4: Histogram of log-tranformed gene expression data of all datasets The mean values of all datasets: mean(c(exprs(single_cell_schelker$eset))) mean(c(racle$expr_mat)) mean(c(schelker_ovarian$expr_mat)) mean(c(hoek$expr_mat)) mean(c(immune_cell_reference$expr_mat)) ## [1] 0.7163828 ## [1] 29.20031 ## [1] 35.24549 ## [1] 51.61823 ## [1] 17.04449 save.image(file=&quot;../results/cache/input_data.rda&quot;, compress=FALSE) References "],
["validitiy-of-single-cell-rna-seq-as-reference.html", "3 Validitiy of single cell RNA-seq as reference 3.1 Compare simululated to genuine bulk samples 3.2 Enrichment analysis: Test for systematic Bias 3.3 Compare predicted fractions 3.4 Correlation with immune reference samples.", " 3 Validitiy of single cell RNA-seq as reference Since we rely on simulated bulk RNA-seq samples generated from scRNA-seq data, we first investigate if this approach is valid and will give reasonable estimates of the methods’s performances. We conduct the following analyses: Compare simulated bulk samples with a corresponding genuine bulk sample. Check if immune-related genes are subject to a systematic bias. Compare the predicted fractions of all methods on bulk vs. simulated samples. Compare simulated bulk samples with a reference immune cell sample of the same cell type. 3.1 Compare simululated to genuine bulk samples provide three samples for which paired single cell and bulk RNA-seq has been performed. We generate simulated bulk RNA-seq samples by taking the average of the single cells and compare the simulated to the genuine samples. bulk_mean = sapply(colnames(schelker_ovarian$expr_mat), function(donor) { ind = pData(single_cell_schelker$eset)$donor == donor apply(exprs(single_cell_schelker$eset)[,ind], 1, mean) }) bulk_mean = apply(bulk_mean, 2, scale_to_million) Figure 3.1: Correlation of simulated bulk samples with corresponding genuine bulk RNA-seq samples. Genes used in the signatures of quanTIseq, EPIC, CIBERSORT and MCP-counter are shown in blue. 3.2 Enrichment analysis: Test for systematic Bias Next, we calculate the sc-bulk log ratio, by \\[log(sc+1)/log(bulk+1)\\], We observe that the sc-bulk log ratio showes asymmetry: more genes show increased sc-aggregate expression rather than decreased sc-aggregate expression. What are these genes that seem to be higher expressed in single-cell aggregates than bulk? We were wondering if there is a systematic bias towards a certain group of genes that tend to be over/underrepresented in single-cell vs. bulk RNA seq. To this end, we take the geometric mean of the log ratio and run a gene set enrichment test with BioQC, using gene ontology (BP and CC terms) as the knowledgebase. geomMean &lt;- function(x) 10^(mean(log10(x))) avgData &lt;- expr_all %&gt;% group_by(gene_symbol) %&gt;% summarise(MeanBulk=geomMean(bulk), MeanSc=geomMean(single_cell_aggregate), MeanScBulkLogRatio=geomMean(ScBulkLogRatio)) ## translate gene symbol into EntrezIds gsIDs &lt;- AnnotationDbi::select(org.Hs.eg.db, avgData$gene_symbol, c(&quot;ENTREZID&quot;), &quot;SYMBOL&quot;) annoAvgData &lt;- avgData %&gt;% inner_join(gsIDs, by=c(&quot;gene_symbol&quot;=&quot;SYMBOL&quot;)) %&gt;% rename(GeneID=ENTREZID) %&gt;% filter(!is.na(GeneID)) %&gt;% dplyr::rename(GeneSymbol = gene_symbol) %&gt;% dplyr::select(GeneID, GeneSymbol, MeanBulk, MeanSc, MeanScBulkLogRatio) ## build up a list of GO gene-sets go &lt;- AnnotationDbi::select(org.Hs.eg.db, annoAvgData$GeneID, c(&quot;GO&quot;), &quot;ENTREZID&quot;) %&gt;% filter(ONTOLOGY %in% c(&quot;BP&quot;,&quot;CC&quot;), EVIDENCE %in% c(&quot;EXP&quot;, &quot;IDA&quot;, &quot;IPI&quot;, &quot;IMP&quot;, &quot;IGI&quot;, &quot;IEP&quot;)) goList &lt;- with(go, split(ENTREZID, GO)) goMatchList &lt;- sapply(goList, function(x) match(x, annoAvgData$GeneID)) goMatchListLen &lt;- sapply(goMatchList, length) goBioQClist &lt;- goMatchList[goMatchListLen&gt;=5 &amp; goMatchListLen&lt;=1000] ## Run two-sided Wilcoxon-Mann-Whitney test using sc-bulk log ratios goBioQCp &lt;- wmwTest(annoAvgData$MeanScBulkLogRatio, goBioQClist, valType=&quot;p.two.sided&quot;) goBioQCq &lt;- wmwTest(annoAvgData$MeanScBulkLogRatio, goBioQClist, valType=&quot;Q&quot;) goBioQClistAnno &lt;- AnnotationDbi::select(GO.db, names(goBioQClist), c(&quot;ONTOLOGY&quot;, &quot;TERM&quot;), &quot;GOID&quot;) goBioQCres &lt;- cbind(goBioQClistAnno, GeneCount=sapply(goBioQClist, length), BioQC.twosided.pvalue=goBioQCp, BioQC.Qvalue=goBioQCq, FDR=p.adjust(goBioQCp, &quot;BH&quot;), Bonferroni=p.adjust(goBioQCp, &quot;bonferroni&quot;)) #TODO table width and round pvalues Below we display the terms that, under moderate stringency of filtering (Benjamini-Hochberg FDR&lt;0.01), shows significant enrichment in either direction. Interstingly, there are many terms that are highly significantly over- or underrepresented in simulated versus genuine bulk RNA-seq samples. We currently have no explanation for that. 3.2.1 Immune-relevant genes are consistent between sc and bulk data. We notice that among these significantly enriched gene-sets above, no gene-sets are directly involved in immune response, cytokine/interleukin/chemokine response. Alternatively, we show these immune-relevant gene-sets and their results by an incomprehensive keyword search, to further demonstrating that they do not show strong difference between sc-aggregated and bulk data. isImmune &lt;- with(goBioQCres, grepl(&quot;immune\\\\b&quot;, TERM, ignore.case=TRUE) | grepl(&quot;inflammation\\\\b&quot;, TERM, ignore.case=TRUE) | grepl(&quot;cytokine\\\\b&quot;, TERM, ignore.case=TRUE) | grepl(&quot;chemokine\\\\b&quot;, TERM, ignore.case=TRUE) | grepl(&quot;interleukin\\\\b&quot;, TERM, ignore.case=TRUE) | grepl(&quot;antigen\\\\b&quot;, TERM, ignore.case=TRUE) | grepl(&quot;macrophage\\\\b&quot;, TERM, ignore.case=TRUE) | grepl(&quot;dendritic cell\\\\b&quot;, TERM, ignore.case=TRUE)) immuneRes &lt;- goBioQCres %&gt;% filter(isImmune) immuneRes %&gt;% arrange(FDR) %&gt;% DT::datatable(options = list(scrollX=&quot;500px&quot;)) %&gt;% DT::formatSignif(columns=c(&quot;BioQC.twosided.pvalue&quot;, &quot;BioQC.Qvalue&quot;, &quot;FDR&quot;, &quot;Bonferroni&quot;)) All in all the bulk expression data and aggregated single-cell data are concordant, using a very rough method (non-parameterized gene-set enrichment test with BioQC), we observe that some classes of genes, such as protein ubiquitination/deubiquitination, spindle organization, and plama membrane genes, show tendency to be over- od under-represented in single-cell aggregates than bulk data. The reason of this observation is not clear. However, we note that, by a limited keyword-search, we found that there is no significant enrichment of gene-ontology terms invovled in immune response that are significantly different between scRNA and bulk data. This further underlines the legitimity of using this particular pair of aggregated single-cell data and paired bulk sequencing data to benchmark different methods. 3.3 Compare predicted fractions Most importantly, we ask how consistent the methods’ predictions are on simulated vs. genuine bulk RNA-seq samples. We run all methods on both datasets and compare the results. Figure 3.2: Correlation of the methods’ predictions on both simulated and genuine bulk RNA-seq samples The same, but only with the cell types we look at: show_cell_types = c(&quot;B cell&quot;, &quot;Dendritic cell&quot;, &quot;Macrophage/Monocyte&quot;, &quot;NK cell&quot;, &quot;T cell CD4+&quot;, &quot;T cell CD4+ (non-regulatory)&quot;, &quot;T cell regulatory (Tregs)&quot;, &quot;T cell CD8+&quot;, &quot;Cancer associated fibroblast&quot;, &quot;Endothelial cell&quot;) all_results_simulated2 = lapply(names(all_results_simulated), function(method) { all_results_simulated[[method]] %&gt;% select(-method, -source) %&gt;% map_result_to_celltypes(show_cell_types, method) %&gt;% as_tibble(rownames=&quot;cell_type&quot;) %&gt;% mutate(method=method, source=&quot;simulated&quot;) }) all_results_bulk2 = lapply(names(all_results_bulk), function(method) { all_results_bulk[[method]] %&gt;% select(-method, -source) %&gt;% map_result_to_celltypes(show_cell_types, method) %&gt;% as_tibble(rownames=&quot;cell_type&quot;) %&gt;% mutate(method=method, source=&quot;bulk&quot;) }) only_cd4 = c(&quot;epic&quot;, &quot;timer&quot;) # methods that do not estimate subtypes of CD4+ T cells # -&gt; we don&#39;t include them in all methods, because it&#39;s a sum of the subtypes # and therefore the corresponding values would be over-represented. all_results2 = bind_rows(all_results_bulk2, all_results_simulated2) %&gt;% # select(cell_type, `7873M`, `7882M`, `7892M`, source, method) %&gt;% gather(donor, fraction, -cell_type, -source, -method) %&gt;% spread(source, fraction) %&gt;% filter(!(cell_type == &quot;T cell CD4+&quot; &amp; !method %in% only_cd4)) res_methods_validity$all_results_mapped = all_results2 ## Warning: Removed 48 rows containing non-finite values (stat_cor). ## Warning: Removed 48 rows containing missing values (geom_point). Figure 3.3: Correlation of the methods’ predictions on both simulated and genuine bulk RNA-seq samples The results show a strong correlation between both datasets and suggest that the approach is valid in general. The poor overlap of xCell is proabably due to the fact that there is little variance between the samples which xCell requires to compute a meaningful score (see their README on GitHub). To demonstrate that this is the case, we run xCell on the same sample, but this time include 50 immune cell reference samples in the run. By adding additional samples, we add additional variance which enables xCell to compute a meaningful score. Figure 3.4: Correlation of xCell’s prediction on both simulated and genuine bulk RNA-seq samples, including additional samples to increase the variance. 3.4 Correlation with immune reference samples. Finally, to demonstrate that the simulated bulk samples are also biologically meaningflu, we generate simulated bulk samples of different immune cell types and correlate them with reference profiles of pure immune cells. Figure 3.5: Correlation of simulated bulk samples of a certain immune cell type (y-axis) with immune cell reference samples (x-axis) In general, the highest correlation is observed between the expression of the sorted cells and the simulated bulk sample. However, the simulated Dendritic cells do not correlate well with any of the reference profiles. "],
["simulation-benchmark.html", "4 Simulation benchmark 4.1 Average fraction of tumour cells 4.2 Create simulated bulk tissues 4.3 Run the deconvolution 4.4 Results", " 4 Simulation benchmark In this chapter, we will use single cell data from Schelker et al. (2017) to create simulated bulk RNAseq samples of which we know the true cell proportions (=artificial gold standard). Using these data, we can assess the performance of immune deconvolution tools. 4.1 Average fraction of tumour cells To obtain representatitive simulated samples, we are interested in the average fraction of tumour cells vs immune cells in a mixture. Figure 4.1: proportion of cell types by tumor sample cancer_cell_param = MASS::fitdistr(cancer_cells$freq, &quot;normal&quot;) The cancer fraction is \\(\\sim\\mathcal{N}(0.33, 0.3)\\). 4.2 Create simulated bulk tissues The fractions of a sample are randomly assigned in the following procedure: Draw a random tumour cell content from the distribution fitted above The first half of the samples will use melanoma cells, the second half ovarian cancer cells. Assign the remaining fraction (=not cancer cells) randomly to the remaining cell types (B cell, T cell CD8+, Melanoma cell, T cell CD4+ (non-regulatory), Macrophage/Monocyte, T cell regulatory (Tregs), Cancer associated fibroblast, Dendritic cell, Endothelial cell, NK cell, PBMC, Ovarian carcinoma cell) Here, we generate a simulated bulk RNA-seq ExpressionSet: set.seed(42) bulk_eset = make_bulk_eset(eset=single_cell_schelker$eset, cell_fractions = cell_fractions, n_cells=500) 4.3 Run the deconvolution 4.4 Results The following methods have absolute (EPIC, quanTIseq) or pseudo-absolute (CIBERSORT abs. mode, xCell) scores, and we test how well they perform in terms of absolute deviation abs_methods ## [1] &quot;cibersort_abs&quot; &quot;epic&quot; &quot;quantiseq&quot; &quot;xcell&quot; We perform an evaluation for the following cell types. Note that some cell types are redundant (T cell CD4+ is a super-category of Tregs and non-regulatory CD4+ T cells). As some methods provide deconvolution only at the CD4+ level, we compare both categories: show_cell_types = c(&quot;B cell&quot;, &quot;Dendritic cell&quot;, &quot;Macrophage/Monocyte&quot;, &quot;NK cell&quot;, &quot;T cell CD4+&quot;, &quot;T cell CD4+ (non-regulatory)&quot;, &quot;T cell regulatory (Tregs)&quot;, &quot;T cell CD8+&quot;, &quot;Cancer associated fibroblast&quot;, &quot;Endothelial cell&quot;) The following methods do not have signatures to quantify “Macrophages/Monocytes”, but only a Macrophage signature. Unfortunately, our single cell dataset does not distinguish between Macrophages and Monocytes. As we are only comparing correlations here, we still benchmark the Macrophage signature on the “Macrophage/Monocyte” data, as an increase in both should also lead to an increase in Macrophages only. To be fair, we label the results accordingly. macrophage_signature_only = c(&quot;epic&quot;, &quot;timer&quot;) Here, we map the results back to the “gold standard”. We aggregate the results of the different methods into a single table and clean it up for further processing. results_with_gold_standard = inner_join(all_results_tidy, gold_standard, by=c(&quot;sample&quot;, &quot;cell_type&quot;)) res_mixing_study$all_results = results_with_gold_standard 4.4.1 Correlation plots Figure 4.2: The figure shows the correlation of predicted vs. known fractions on 100 simulated bulk RNA seq samples. 4.4.2 Calculate correlations for each method and cell type correlations_tab = correlations %&gt;% select(cell_type, method, pearson) %&gt;% spread(cell_type, pearson) write_tsv(correlations_tab, &quot;../results/tables/mixing_study_correlations.tsv&quot;, na=&quot;&quot;) Figure 4.3: Correlations of predicted vs. known fractions on 100 simulated bulk RNA-seq samples, organized by cell type. 4.4.3 Absolute deviation Next, we were interested in the absolute deviation of the values for methods that compute absolute scores. We assess the absolute deviation using two measures: by fitting a linear model to the values and denoting the slope. If the absolute quantification was perfect, the slope would equal 1. Values &lt; 1 indicate an underprediction and values &gt; 1 an overprediction of the respective cell type. by calculating the root mean square error (RMSE) Note that only EPIC and quanTIseq provide scores that can be interpreted as a cell fraction. xCell and CIBERSORT abs. scale the output score to be ‘absolute’: xCell does an attempt to make the scores resemble percentages, but it is a hard problem, and is very platform and experiment specific. (xCell on github Absolute mode scales relative cellular fractions into a score of arbitrary units that reflects the absolute proportion of each cell type in a mixture. Although not currently expressed as a fraction, the absolute score can be directly compared across cell types (i.e., relative differences between cell types are maintained) (cibersort FAQ) Also note that these values are not necessarily accurate for quanTIseq and EPIC, which account for the different mRNA contents of different cell types. As single cell data is already normalized on a per-cell level, the scaling factors are most likely not appropriate with the simulated data. This is why we use the validation datasets to generate the deviation plots in the main paper. Figure 4.4: Absolute deviation of the predictions represented as the slope of a linear model. Confidence intervals are computed using confint on the result of lm. Figure 4.5: Absolute deviation of the predictions represented as RMSE. References "],
["validation-with-real-data.html", "5 Validation with real data 5.1 Between- and within-sample comparisons 5.2 Within-sample comparison 5.3 Between-sample comparisons", " 5 Validation with real data We use datasets which estimate the immune cell proportions using flow cytometry as an additional validation for our simulation benchmark. We use the following three validation datasets (see section 2): datasets = list( racle=racle, hoek=hoek, schelker_ovarian=schelker_ovarian ) We use the following cell types, which are available in (some of) the datasets use_cell_types = c(&quot;T cell&quot;, &quot;T cell CD8+&quot;, &quot;T cell CD4+&quot;, &quot;Monocyte&quot;, &quot;B cell&quot;, &quot;Dendritic cell&quot;, &quot;NK cell&quot;) Here, we combine the predictions with the ‘gold standard’ reference data. all_results_ref = inner_join(all_results, all_refs, by = c(&quot;sample&quot; = &quot;sample&quot;, &quot;dataset&quot; = &quot;dataset&quot;, &quot;cell_type&quot; = &quot;cell_type&quot;)) res_validation$all_results = all_results_ref The scores of the methods have different properties and not all of them are directly comparable. We distinguish between three types of comparisons: absolute scores: allow to compare within and between samples. scores relative to the total immune cell content: allow to compare within a sample scores in arbitrary units, that only allow to compare between samples. Here, we assign the methods to their respective group: abs_methods = c(&quot;cibersort_abs&quot;, &quot;quantiseq&quot;, &quot;epic&quot;) within_methods = c(&quot;cibersort&quot;, &quot;cibersort_abs&quot;, &quot;quantiseq&quot;, &quot;epic&quot;) between_methods = c(&quot;cibersort_abs&quot;, &quot;quantiseq&quot;, &quot;epic&quot;, &quot;timer&quot;, &quot;xcell&quot;, &quot;mcp_counter&quot;) 5.1 Between- and within-sample comparisons Only works for methods providing an absolute score (cibersort_abs, quantiseq, epic). All other methods are included for reference. Figure 5.1: Comparison of absolute predictions for three validation dataset. Next, we assess the absolute deviation of the methods. Like in the previous section, we use two measures: The slope of a linear model fitted to the data. The root mean square error. Figure 5.2: Comparison of absolute methods. Values indicate the slope of a linear model fitted to predictions vs. true fractions. Values &lt; 1 indicate an under-prediction of the cell type, values &gt; 1 an over-prediction respectively. The error bars have been computed using confint on the result of lm. Figure 5.3: Comparison of absolute methods. The values show the RMSE 5.2 Within-sample comparison only works with methods that provide an absolute score, or a score that is relative to total immune cell content (cibersort, cibersort_abs, quantiseq, epic). Figure 5.4: Comparison of predictions within each individual sample Compute the average over all samples: Figure 5.5: Correlations of within-sample comparisons. The last column shows the mean over all samples. 5.3 Between-sample comparisons For this, we need to look at every cell type independently. Works for all methods except CIBERSORT. Figure 5.6: Correlations of known vs. predicted fractions for each cell type independenctly. The ‘Tcell’ column corresponds to the amount of profiled total T cells or the sum of CD4+ and CD8+ T cells respectively. Compute the average over all cell types. We only use cell types with at least 5 samples to obtain reasonable correlation estimates. We also exclude the T-cell supercategory to avoid redundancies. use_cell_types2 = c(&quot;B cell&quot;, &quot;Dendritic cell&quot;, &quot;Macrophage/Monocyte&quot;, &quot;NK cell&quot;, &quot;T cell CD4+&quot;, &quot;T cell CD8+&quot;) Figure 5.7: Performance on validation datasets by cell type. The last column shows the mean over all cell types. Note that, although the correlation for CD8+ T cells looks bad, it does not necessarily mean the predictions are bad. There is just little variance between the samples and CD8+ T cell abundance is generally low, which is correlctly predicted by the methods. "],
["detection-limit-and-false-positive-predictions.html", "6 Detection limit and false positive predictions 6.1 Predictions with increasing immune cell content 6.2 Detection Limit 6.3 False-positive predictions", " 6 Detection limit and false positive predictions How many cells of a certain type do we need for a method to detect immune cell infiltration (=Detection limit)? How many cells of a certain type are detected, although we know they are not there (=false positives)? We again use the single cell dataset to simulate samples that consist of background cells (i.e. non-immune cells: fibroblasts, endothelial cells cancer cells) and add an increasing amount of immune cells of a certain type. We define the detection limit as the minimal fraction at which the method the abundance of the cell type to be significantly different from zero. We define false positives as the predicted fraction of a certain cell type at zero inflitration level. For each amount of immune cells, we generate 5 random samples to compute a confidence interval. We use the following cell types: show_cell_types = c(&quot;B cell&quot;, &quot;Dendritic cell&quot;, &quot;Macrophage/Monocyte&quot;, &quot;NK cell&quot;, &quot;T cell CD4+&quot;, &quot;T cell CD8+&quot;) 6.1 Predictions with increasing immune cell content Figure 6.1: We add an increasing amount of each cell type independently to the 1,800 background cells. The figure shows the predictions for each cell type and method at increasing infiltration levels. The following about false positives and the detection limit are directly derived from the data shown in the plot above. 6.2 Detection Limit Figure 6.2: Detection limit of the methods per cell type. The values indicate the minimal percentage of infiltration at which the method first reliably detects the cell type. 6.3 False-positive predictions Predicted amount of cells when there are none. This analysis is based on all of the data above where only background cells are present, i.e. fraction of immune cells = 0. Figure 6.3: Predicted amount of a certain cell type while it is actually absent. "],
["spillover-analysis.html", "7 Spillover Analysis 7.1 Complete Spillover Matrix 7.2 Summary figure: Signal to noise ratio 7.3 Summary figure: migration charts 7.4 Investigate marker genes 7.5 Check expression of markers in “detection limit” simulation dataset. 7.6 Deconvolution results before and after", " 7 Spillover Analysis In this chapter, we investigate which other cell types a method predicts, if there is actually only a certain cell type present. In FACS, this phenomenon is known as “spillover”. To this end, we use three datasets immune reference: bulk RNA seq profiles from sorted immune cells (=quanTIseq training data) artificial bulk: simulated bulk RNA seq profiles from single cells (e.g. only T cells) artificial bulk with background: simulated bulk RNA seq profiles from single cells with ~80% other cells (cancer, fibroblasts, …) We test the following cell types: show_cell_types = c(&quot;B cell&quot;, &quot;Dendritic cell&quot;, &quot;Macrophage/Monocyte&quot;, &quot;NK cell&quot;, &quot;T cell CD4+&quot;, &quot;T cell CD8+&quot;) 7.1 Complete Spillover Matrix Figure 7.1: This figure shows the spillover for all methods, cell types and datasets 7.2 Summary figure: Signal to noise ratio Noise is defined as the sum of all predictions of other cell types than the one that is actually present. Signal is defined as the predicted fraction of the cell type that is actually present. The signal ratio is defined as \\(\\frac{\\text{signal}}{\\text{signal+noise}}\\). Higher values indicate less noise (i.e. predictions of cell types that are not there). Figure 7.2: Signal ratios for each cell type and dataset. 7.3 Summary figure: migration charts Download this figure as high quality pdf Figure 7.3: migration charts for all three dataset. The ‘flow’ indicates which cell types have been predicted instead of the truly abundant cell type. The value in the middle of the chart indicates the ‘noise ratio’, i.e. the sum of all false positive predictions. 7.4 Investigate marker genes Spillover appears to primarily happen between CD8+ and CD4+ T cells and from DCs to monocytes/macrophages and B cells. We were particularly concerned by the spillover between DCs and B cells as we could not validate it in the ‘immune cell reference’ dataset. In this section, we demonstrate that the single cell populations of DCs and B cells are both distinct and well-defined and, next, identify a handful of marker genes that are specific for both plasmacytoid DCs and B cells and drive the spillover. First, we suspected that the effect might be due to misannotations in the single cell dataset. Here, we show the B cells and DCs in the t-SNE plot of the single cell dataset. Figure 7.4: B cell and DC clusters in the single cell dataset. We observe that both clusters are distinct and non-overlapping. Next, we look for the expression of typical marker genes of DCs and B cells. ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] ## ## [[6]] ## ## [[7]] Conclusion: The B cell and DC cluster separate well, the marker genes do not overlap. The DC cluster are plasmacytoid DCs, mDCs are somewhere hidden in the Macrophage/Monocyte cluster and cannot be distinguished. 7.5 Check expression of markers in “detection limit” simulation dataset. markers = list( &quot;B cell&quot; = c(&quot;MS4A1&quot;, #=CD20 &quot;CD19&quot;, &quot;CD22&quot; ), &quot;myleoid&quot; = c(&quot;ITGAM&quot;, &quot;ITGAX&quot;), #=CD11B,CD11C &quot;MHC class II&quot; = c(&quot;HLA-DRB1&quot;, &quot;HLA-DRA&quot;), &quot;plasmacytoid DC&quot; = c(&quot;CLEC4C&quot;, &quot;IL3RA&quot;)) We check the abundance of the marker genes in the simulated samples of the dataset we used for assessing the false positive predictions and the detection limit. Figure 7.5: Correlation of marker genes with increase of the amount of a certain cell type. Along x-axis: cell type used for simulation. Along y-axis: marker genes. Figure 7.6: Even though the number of dendritic cells does correlate with MS4A1, a B cell marker, the absolute expression is really low. This effect can be driven by few misclassified cells but cannot explain the spillover effects we observe. 7.5.1 Expression of marker genes in the cell populations For EPIC, quanTIseq and CIBERSORT, we retreive a list of all genes included in the signature matrices. # quanTIseq til10 = read_tsv(&quot;../immunedeconv/inst/extdata/quantiseq/TIL10_signature.txt&quot;) %&gt;% as.data.frame() %&gt;% column_to_rownames(&quot;ID&quot;) # EPIC tref = EPIC::TRef$refProfiles[EPIC::TRef$sigGenes,] # CIBERSORT lm22 = read_tsv(&quot;../lib/CIBERSORT/LM22.txt&quot;) %&gt;% as.data.frame() %&gt;% column_to_rownames(&quot;Gene symbol&quot;) For each marker gene, we show the expression in all cell type populations in the single cell dataset. We sort the list by the product of the mean expression in B cells and Dendritic cells to obtain a ranked list of genes that are enriched in both B cells and DCs. i The plots are very large, that’s why we don’t include them directly. You can view the plots in full quality as PDF under the following links: EPIC quanTIseq CIBERSORT The figures show the expression of marker genes across cell types. Along x-axis: cell types used for simulation. Along y-axis: signature genes for the cell types derived from the signature matrices of the deconvolution methods. 7.6 Deconvolution results before and after We checked the most promising candidate genes on genecards.org, and took note of those that were indeed annotated as being specific for both B cells and plasmacytoid dendritic cells in the LifeMap Discovery database. B_pDC_genes = c(&quot;TCL1A&quot;, &quot;TCF4&quot;, &quot;CD37&quot;, &quot;SPIB&quot;, &quot;BCL11A&quot;, &quot;IRF8&quot;) To assess the effects on deconvolution, we generate 10 simulated dendritic cell samples, run the deconvolution once with default options and once with the marker genes removed, and check the ‘spillover’ to B cells. Figure 7.7: Predictions on 10 simulated DC samples before and after removal of the six genes. "],
["creating-publication-ready-figures.html", "8 Creating publication ready figures 8.1 Benchmark results 8.2 detection limit / false positive figure 8.3 Migration charts for Spillover analysis", " 8 Creating publication ready figures fun_breaks3 = function(limits) { breaks = signif(max(limits) * c(0.25, 0.75),1) names(breaks) = attr(breaks, &quot;labels&quot;) breaks } tmp_cor = res_methods_validity$all_results_mapped %&gt;% group_by(method) %&gt;% do(make_cor(.$bulk, .$simulated)) %&gt;% mutate(pearson=round(pearson, 2)) %&gt;% inner_join(method_names) res_methods_validity$all_results_mapped %&gt;% inner_join(cell_type_names) %&gt;% inner_join(method_names) %&gt;% ggplot(aes(x=bulk, y=simulated)) + geom_point(aes(colour=cell_type_name), size=.5) + facet_wrap(~method_name, scales=&quot;free&quot;, nrow=1) + theme(legend.position = &quot;top&quot;) + geom_text(data=tmp_cor, mapping=aes(label=paste0(&quot;r=&quot;, pearson), x=0, y=0), hjust=0, vjust=-8, size=2.4) + panel_border() + scale_x_continuous(breaks=fun_breaks3) + scale_y_continuous(breaks=fun_breaks3) + theme(strip.text=element_text(size=6), panel.spacing = unit(2, &quot;mm&quot;), axis.text = element_text(size=8)) fun_breaks = function(limits) { breaks = signif(max(limits) * c(0.25, 0.75),1) names(breaks) = attr(breaks, &quot;labels&quot;) breaks } corr_annot = res_mixing_study$correlations %&gt;% select(method, cell_type, pearson) %&gt;% mutate(pearson = round(pearson, 2)) %&gt;% inner_join(method_names) %&gt;% inner_join(cell_type_names) %&gt;% distinct() plot_mixing = res_mixing_study$all_results %&gt;% inner_join(method_names) %&gt;% inner_join(cell_type_names) %&gt;% ggplot(aes(x=true_fraction, y=estimate)) + geom_point(size=.2) + geom_text(data=corr_annot, mapping=aes(label=paste0(&quot;r=&quot;, pearson), x=-Inf, y=-Inf), hjust=-0.1, vjust=-8, size=2.4) + facet_grid(method_name ~ cell_type_name, scales=&quot;free&quot;) + stat_smooth(aes(color=method), method=&quot;lm&quot;, size=.4) + scale_color_manual(values=color_scales$methods, na.value=&quot;grey&quot;) + scale_x_continuous(breaks=c(.2)) + scale_y_continuous(breaks=fun_breaks) + theme(legend.position = &quot;none&quot;, strip.text=element_text(size=6), panel.spacing = unit(.5, &quot;mm&quot;), axis.text = element_text(size=8)) + ylab(&quot;estimated fraction&quot;) + xlab(&quot;true fraction&quot;) + panel_border() ggsave(&quot;../results/figures/correlations_mixing.pdf&quot;, width=WIDTH, height=140, units = &quot;mm&quot;) ggsave(&quot;../results/figures/correlations_mixing.png&quot;, width=WIDTH, height=140, units = &quot;mm&quot;, dpi=600) 8.1 Benchmark results Figure 8.1: Benchmark results main figure. 8.2 detection limit / false positive figure data = res_sensitivity$all_results %&gt;% filter(cell_type == input_cell_type) %&gt;% mutate(frac_immune_cells = as.numeric(frac_immune_cells)/100) %&gt;% filter(frac_immune_cells &lt; .4) %&gt;% inner_join(method_names) %&gt;% inner_join(cell_type_names) data_detection_limit = res_sensitivity$sensitivity %&gt;% rename(cell_type = input_cell_type) %&gt;% inner_join(method_names) %&gt;% inner_join(cell_type_names) %&gt;% mutate(min_frac = as.numeric(min_frac)/100) data_false_positives = res_sensitivity$specificity %&gt;% group_by(method, cell_type) %&gt;% summarise(fp_prediction = mean(estimate)) %&gt;% inner_join(method_names) %&gt;% inner_join(cell_type_names) fun_breaks2 = function(limits) { breaks = signif(max(limits) * c(0, 0.5),1) names(breaks) = attr(breaks, &quot;labels&quot;) breaks } data %&gt;% ggplot(aes(x=frac_immune_cells, y=mean)) + geom_ribbon(aes(ymin=mean-ci, ymax=mean+ci), alpha=.2) + geom_point(size=.2, shape=4) + # geom_errorbar(aes(ymin=mean-ci, ymax=mean+ci)) + panel_border() + facet_grid(method_name ~ cell_type_name, scales = &quot;free_y&quot;) + scale_x_continuous(breaks=c(.1, .3)) + scale_y_continuous(breaks=fun_breaks2) + geom_vline(data=data_detection_limit, mapping=aes(xintercept=min_frac, colour=&quot;detection limit&quot;)) + geom_hline(data=data_false_positives, mapping=aes(yintercept=fp_prediction, colour=&quot;false positive fraction&quot;)) + ylab(&quot;average estimate&quot;) + xlab(&quot;fraction of spike-in cells&quot;) + theme(legend.position = &quot;top&quot;, strip.text=element_text(size=6), panel.spacing = unit(1, &quot;mm&quot;), axis.text = element_text(size=8)) ## Warning: Removed 3 rows containing missing values (geom_vline). 8.3 Migration charts for Spillover analysis Figure 8.2: Migration chart figure for paper. "],
["references.html", "9 References", " 9 References "]
]
